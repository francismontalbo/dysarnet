{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MODEL EVALUATOR</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DEPENDENCIES\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools, scikitplot\n",
    "import os, cv2, glob, time, pickle, logging\n",
    "\n",
    "from keras_flops import get_flops\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from keras.utils.layer_utils import count_params\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from matplotlib.ticker import FuncFormatter, PercentFormatter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, mean_squared_error, \n",
    "                             mean_squared_log_error, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "\n",
    "# PREVENT ERROR UNCESSARY MESSAGES\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "cwd = os.getcwd()\n",
    "main_ds = os.path.join(cwd, 'ds')\n",
    "\n",
    "task = ['binary', 'severity']\n",
    "ds_folder = ['ua', 'torgo']\n",
    "\n",
    "task_idx, ds_folder_idx = 1, 1  # Change to select binary or severity task, and 'torgo' dataset (0 for 'ua' dataset)\n",
    "\n",
    "selected_ds = os.path.join(main_ds, ds_folder[ds_folder_idx], f\"{ds_folder[ds_folder_idx]}_{task[task_idx]}\")\n",
    "print(f\"{selected_ds=}\")\n",
    "\n",
    "train_data_dir = os.path.join(selected_ds, 'train')\n",
    "validation_data_dir = os.path.join(selected_ds, 'val')\n",
    "test_data_dir = os.path.join(selected_ds, 'test')\n",
    "\n",
    "print(\"*\" * 55)\n",
    "print(\"Data folders found!\")\n",
    "print(\"*\" * 55)\n",
    "print(f\"Train Path: {train_data_dir}\")\n",
    "print(f\"Validation Path: {validation_data_dir}\")\n",
    "print(f\"Test Path: {test_data_dir}\")\n",
    "print(\"*\" * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_name = \"DySARNet\"\n",
    "model_name = f\"{architecture_name}_{task[task_idx]}_{ds_folder[ds_folder_idx]}\"\n",
    "model_path = os.path.join('model/', model_name)\n",
    "\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "batch_size = 8\n",
    "img_rows, img_cols = 224, 224\n",
    "preprocessing = ImageDataGenerator(rescale = 1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATORS\n",
    "if task_idx == 0:\n",
    "    classes = ['0_Normal', '1_Dysarthria']\n",
    "    labels = ['Normal', 'Dysarthria']\n",
    "else:\n",
    "    classes = ['0_Normal', '1_Mild', '2_Moderate', '3_SeverelyModerate', '4_Severe']\n",
    "    labels = ['Normal', 'Mild', 'Moderate', 'Severly Moderate', 'Severe']\n",
    "\n",
    "val_datagen = preprocessing\n",
    "\n",
    "test_datagen = preprocessing\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        classes=classes)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        classes=classes)\n",
    "\n",
    "# CHECK  THE NUMBER OF SAMPLES\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "print(\"Validation samples:\", nb_validation_samples)\n",
    "print(\"Test samples:\", nb_test_samples)\n",
    "print(f\"TOTAL SAMPLES: {(nb_validation_samples + nb_test_samples)}\")\n",
    "\n",
    "if nb_validation_samples == 0:\n",
    "    print(\"NO DATA VALIDATION FOUND! Please check your validation data path and folders!\")\n",
    "    print(\"Check the data folders first!\")\n",
    "else:\n",
    "    print(\"Validation samples found!\")\n",
    "    \n",
    "if nb_test_samples == 0:\n",
    "    print(\"NO DATA TEST FOUND! Please check your test data path and folders!\")\n",
    "    print(\"Check the data folders first!\")\n",
    "else:\n",
    "    print(\"Test samples found!\")\n",
    "\n",
    "# check the class indices\n",
    "validation_generator.class_indices\n",
    "test_generator.class_indices\n",
    "\n",
    "# true labels\n",
    "Y_test=validation_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "num_classes= len(validation_generator.class_indices)\n",
    "\n",
    "if nb_validation_samples and nb_test_samples > 0:\n",
    "    print(\"Generators are set!\")\n",
    "    print(\"Check if dataset is complete and has no problems before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = 'pretrained_models'\n",
    "model_path = os.path.join(model_directory, model_name)\n",
    "\n",
    "model_file_pattern = os.path.join(model_path, f'{model_name}-*.h5')\n",
    "history_file_pattern = os.path.join(model_path, f'{model_name}-*.history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all model files and history files matching the pattern\n",
    "model_files = glob.glob(model_file_pattern)\n",
    "history_files = glob.glob(history_file_pattern)\n",
    "print(f\"Searching for models at path: {model_file_pattern}\")\n",
    "\n",
    "if len(model_files) == 0:\n",
    "    print(f\"No models found for {model_name}\")\n",
    "else:\n",
    "    for model_file in model_files:\n",
    "        # Load the model\n",
    "        model = load_model(model_file)\n",
    "        loaded_file = os.path.basename(model_file)\n",
    "        print(f\"The model {loaded_file} is loaded\")\n",
    "        \n",
    "if len(history_files) == 0:\n",
    "    print(f\"No history files found for {model_name}\")\n",
    "else:\n",
    "    for history_file in history_files:\n",
    "        # Load the history file\n",
    "        with open(history_file, 'rb') as f:\n",
    "            history = pickle.load(f)\n",
    "        loaded_history = os.path.basename(history_file)\n",
    "        print(f\"The history {loaded_history} is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the Cost-efficiency via FLOPS.\n",
    "flops  = float(\"{0:.2f}\".format(get_flops(Model(model.input, model.output), batch_size=1)/ 10 ** 9))\n",
    "params = float(\"{0:.2f}\".format(model.count_params() / 10 ** 6))\n",
    "trainable_count = float(\"{0:.2f}\".format(count_params(model.trainable_weights) / 10 ** 6))\n",
    "print(f\"{model_name} FLOPS and Parameters:\")\n",
    "print('-'*25)\n",
    "print(\"FLOPS:\", flops, \"GFLOPS\")\n",
    "print(\"Params:\", params, \"M\")\n",
    "print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model performance.\n",
    "print(f\"{model_name} is Validating....\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "val_scores = model.evaluate(validation_generator, return_dict=True, verbose=1)\n",
    "\n",
    "val_acc = val_scores['accuracy'] * 100\n",
    "val_loss = val_scores['loss'] * 100\n",
    "\n",
    "print(f\"Val accuracy: {val_acc:.2f}%\")\n",
    "print(f\"Val loss: {val_loss:.2f}%\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "val_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(\"\\nValidation Time:\")\n",
    "print(\"-\"*15)\n",
    "print(f\"Elapsed time in seconds: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elapsed time in minutes: {elapsed_time / 60:.2f} minutes\")\n",
    "print(f\"Elapsed time in hours: {elapsed_time / 3600:.2f} hours\")\n",
    "print()\n",
    "print(f\"Total Validation Time: {val_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model performance.\n",
    "print(f\"{model_name} is Testing....\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_scores = model.evaluate(test_generator, return_dict=True, verbose=1)\n",
    "\n",
    "test_acc = test_scores['accuracy'] * 100\n",
    "test_loss = test_scores['loss'] * 100\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Test loss: {test_loss:.2f}%\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "test_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(\"\\nTest Time:\")\n",
    "print(\"-\"*15)\n",
    "print(f\"Elapsed time in seconds: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elapsed time in minutes: {elapsed_time / 60:.2f} minutes\")\n",
    "print(f\"Elapsed time in hours: {elapsed_time / 3600:.2f} hours\")\n",
    "print()\n",
    "print(f\"Total Test Time: {test_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference the model performance with validation data.\n",
    "print(f\"{model_name} is Inferencing with the Validation Data....\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred = model.predict(validation_generator, \n",
    "                       nb_validation_samples // batch_size, \n",
    "                       workers=1, \n",
    "                       verbose=1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "val_inference_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(\"\\nValidation Inference Time:\")\n",
    "print(\"-\"*15)\n",
    "print(f\"Elapsed time in seconds: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elapsed time in minutes: {elapsed_time / 60:.2f} minutes\")\n",
    "print(f\"Elapsed time in hours: {elapsed_time / 3600:.2f} hours\")\n",
    "print()\n",
    "print(f\"Total Test Time: {val_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference the model performance with test data.\n",
    "print(f\"{model_name} is Inferencing with the Test Data....\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_pred = model.predict(test_generator, \n",
    "                          nb_validation_samples/batch_size, \n",
    "                          workers=1, \n",
    "                          verbose=1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "test_inference_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "\n",
    "print(\"\\nTest Inference Time:\")\n",
    "print(\"-\"*15)\n",
    "print(f\"Elapsed time in seconds: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elapsed time in minutes: {elapsed_time / 60:.2f} minutes\")\n",
    "print(f\"Elapsed time in hours: {elapsed_time / 3600:.2f} hours\")\n",
    "print()\n",
    "print(f\"Total Test Time: {test_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the test accuracy percentage to name the model\n",
    "\n",
    "test_acc = test_scores['accuracy']\n",
    "accuracy_percentage = \"{:.2%}\".format(test_acc)\n",
    "\n",
    "print(f'The model {model_name} achieved a test accuracy of: {accuracy_percentage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure save point.\n",
    "\n",
    "# Define the figure save point.\n",
    "fig_save_point = f'figures/{task[task_idx].title()}/{ds_folder[ds_folder_idx].upper()}/{model_name}/'\n",
    "\n",
    "# Check if the directory exists, create it if not.\n",
    "if not os.path.exists(fig_save_point):\n",
    "    os.makedirs(fig_save_point)\n",
    "    print(f\"Directory '{fig_save_point}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{fig_save_point}' already exists, no new folder is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image saving format and DPI.\n",
    "dpi = 300\n",
    "fformat = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and subplot with specified size\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "# Loss Curves\n",
    "ax.plot(history['loss'], '#0F52BA', linewidth=3.0, marker='8')\n",
    "ax.plot(history['val_loss'], '#800020', linewidth=3.0, ls='--')\n",
    "\n",
    "# Accuracy Curves\n",
    "ax.plot(history['accuracy'], '#006D77', linewidth=3.0, marker='8')\n",
    "ax.plot(history['val_accuracy'], '#C28800', linewidth=3.0, ls='--')\n",
    "\n",
    "# Add shaded area for loss error\n",
    "loss_error = np.std(history['val_loss'])\n",
    "ax.fill_between(range(len(history['val_loss'])), \n",
    "                history['val_loss']-loss_error, \n",
    "                history['val_loss']+loss_error, color='#800020', alpha=0.2)\n",
    "\n",
    "# Add shaded area for accuracy error\n",
    "acc_error = np.std(history['val_accuracy'])\n",
    "ax.fill_between(range(len(history['val_accuracy'])), \n",
    "                history['val_accuracy']-acc_error, \n",
    "                history['val_accuracy']+acc_error, \n",
    "                color='#C28800', \n",
    "                alpha=0.2)\n",
    "\n",
    "# Set labels, titles, and grid\n",
    "ax.set_xlabel('Epochs', fontsize=16)\n",
    "ax.set_ylabel('Accuracy/Loss', fontsize=16)\n",
    "ax.grid(True, linewidth=1)\n",
    "ax.tick_params(width=2)\n",
    "# ax.set_title('Accuracy and Loss Curves', fontsize=16)\n",
    "\n",
    "# Format y-axis labels as percentages\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "# Set legend and linewidths\n",
    "ax.legend(['Train Loss', 'Val Loss', 'Train Acc', 'Val Acc'], fontsize=8, loc='best')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_save_point + '0-' + model_name + '_' + accuracy_percentage + '-' + 'combined_loss_curves.svg', format=fformat, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and subplots with specified size\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Loss Curves\n",
    "ax[0].plot(history['loss'],'#ff7f0e',linewidth=3.0, marker='8')\n",
    "ax[0].plot(history['val_loss'],'#1f77b4',linewidth=3.0, ls='--')\n",
    "\n",
    "# Add shaded area for loss error\n",
    "loss_error = np.std(history['val_loss'])\n",
    "ax[0].fill_between(range(len(history['val_loss'])), \n",
    "                   history['val_loss']-loss_error, \n",
    "                   history['val_loss']+loss_error, color='#ff7f0e', alpha=0.2)\n",
    "\n",
    "\n",
    "ax[0].legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "ax[0].set_xlabel('Epochs ',fontsize=10)\n",
    "ax[0].set_ylabel('Loss',fontsize=10)\n",
    "ax[0].set_title('Loss Curves',fontsize=12)\n",
    "ax[0].grid(True, linewidth=1)\n",
    "ax[0].tick_params(width=2)\n",
    "\n",
    "for spine in ax[0].spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Accuracy Curves\n",
    "ax[1].plot(history['accuracy'],'#ff7f0e',linewidth=3.0, marker='8')\n",
    "ax[1].plot(history['val_accuracy'],'#1f77b4',linewidth=3.0, ls='--')\n",
    "\n",
    "# Add shaded area for accuracy error\n",
    "acc_error = np.std(history['val_accuracy'])\n",
    "ax[1].fill_between(range(len(history['val_accuracy'])), \n",
    "                   history['val_accuracy']-acc_error, \n",
    "                   history['val_accuracy']+acc_error, \n",
    "                   color='#ff7f0e', \n",
    "                   alpha=0.2)\n",
    "\n",
    "ax[1].legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "ax[1].set_xlabel('Epochs',fontsize=10)\n",
    "ax[1].set_ylabel('Accuracy',fontsize=10)\n",
    "ax[1].set_title('Accuracy Curves',fontsize=12)\n",
    "ax[1].grid(True, linewidth=1)\n",
    "ax[1].tick_params(width=2)\n",
    "ax[1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "for spine in ax[1].spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "# fig.savefig(fig_save_point + model_name + '_' + accuracy_percentage + '_' + 'loss_acc_curves.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The confusion matrix method\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "if len(labels) == 5:\n",
    "    labels = ['0', '1', '2', '3', '4']\n",
    "else:\n",
    "    labels = ['0', '1']\n",
    "\n",
    "def plot_confusion_matrix(cm, fontsize=16, classes=labels, normalize=True, title=model._name + ' Validation', cmap=plt.cm.bone, ax=None, filename=None):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Dynamically adjust the height of the colorbar based on the height of the confusion matrix plot\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05*(10./len(classes)))\n",
    "    ax.figure.colorbar(im, cax=cax)\n",
    "    cbar = ax.figure.colorbar(im, cax=cax)\n",
    "    cbar.ax.yaxis.set_label_coords(1.8, 0.5)\n",
    "    \n",
    "    # Change the font size of the color bar\n",
    "    cbar.ax.tick_params(labelsize=12, rotation=90) \n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, rotation=0, ha='center', fontsize=fontsize)\n",
    "    ax.set_yticklabels(classes, rotation=0, ha='center', fontsize=fontsize)\n",
    "#     ax.tick_params(axis='both', which='major', labelsize=12, pad=5)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if cm[i, j] > thresh else \"white\", \n",
    "                 fontsize=fontsize, ha='center')\n",
    "    \n",
    "    ax.set_ylabel('True label', fontsize=fontsize)\n",
    "    ax.set_xlabel('Predicted label', fontsize=fontsize)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    if filename:\n",
    "        fig.savefig(filename, format='svg')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the validation results using a confusion matrix.\n",
    "\n",
    "target_names = classes\n",
    "print(classification_report(Y_test,y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, \n",
    "                            digits=4))\n",
    "\n",
    "cm_validation = confusion_matrix(Y_test, y_pred.argmax(axis=-1))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "\n",
    "val_cm_plot = plot_confusion_matrix(cm_validation, \n",
    "                                    classes=labels,\n",
    "                                    normalize=False,\n",
    "                                    title='',\n",
    "                                    ax=ax1)\n",
    "\n",
    "val_cm_plot_normalized = plot_confusion_matrix(cm_validation, \n",
    "                                               classes=labels,\n",
    "                                               normalize=True,\n",
    "                                               title='',\n",
    "                                               ax=ax2)\n",
    "\n",
    "# fig.savefig(fig_save_point + model_name + '_' + accuracy_percentage + '_' + 'val_cm_both.svg', format='svg')\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "val_cm_plot = plot_confusion_matrix(cm_validation, \n",
    "                                    classes=labels,\n",
    "                                    normalize=False,\n",
    "                                    title='',\n",
    "                                    ax=ax1)\n",
    "\n",
    "fig.savefig(fig_save_point + '1-' + model_name + '_' + accuracy_percentage + '-' + 'val_cm.svg', format=fformat, dpi=dpi)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "val_cm_plot_normalized = plot_confusion_matrix(cm_validation, \n",
    "                                               classes=labels, \n",
    "                                               normalize=True,\n",
    "                                               title='',\n",
    "                                               ax=ax1)\n",
    "\n",
    "fig.savefig(fig_save_point + '1-' + model_name + '_' + accuracy_percentage + '-' + 'val_cm_norm.svg', format=fformat, dpi=dpi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test results using a confusion matrix\n",
    "print(classification_report(test_labels,test_pred.argmax(axis=-1),\n",
    "                            target_names=classes, \n",
    "                            digits=4))\n",
    "\n",
    "cm_test = confusion_matrix(test_labels, test_pred.argmax(axis=-1))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "\n",
    "test_cm_plot = plot_confusion_matrix(cm_test, \n",
    "                                    classes=labels, \n",
    "                                    normalize=False,\n",
    "                                    title='', \n",
    "                                    ax=ax1)\n",
    "\n",
    "test_cm_plot_normalized = plot_confusion_matrix(cm_test, \n",
    "                                               classes=labels,\n",
    "                                               normalize=True,\n",
    "                                               title='', \n",
    "                                               ax=ax2)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "test_cm_plot = plot_confusion_matrix(cm_test, \n",
    "                                    classes=labels, \n",
    "                                    normalize=False, \n",
    "                                    title='', \n",
    "                                    ax=ax1)\n",
    "\n",
    "fig.savefig(fig_save_point + '2-' + model_name + '_' + accuracy_percentage + '-' + 'test_cm.svg', format=fformat, dpi=dpi)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "test_cm_plot_normalized = plot_confusion_matrix(cm_test, \n",
    "                                               classes=labels, \n",
    "                                               title='',\n",
    "                                               normalize=True,\n",
    "                                               ax=ax1)\n",
    "\n",
    "fig.savefig(fig_save_point + '2-' + model_name + '_' + accuracy_percentage + '-' + 'test_cm_norm.svg', format=fformat, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the ROC and PR curves for the val data.\n",
    "\n",
    "# Set font properties\n",
    "font = FontProperties()\n",
    "font.set_family('Tahoma')\n",
    "font.set_size(18)\n",
    "\n",
    "# Plot ROC curve for validation set\n",
    "scikitplot.metrics.plot_roc(Y_test, y_pred)\n",
    "# plt.title(model._name + ' Validation ROC Curve', fontproperties=font)\n",
    "plt.title('')\n",
    "plt.xlabel('Specificity', fontproperties=font)\n",
    "plt.ylabel('Sensitivity', fontproperties=font)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_save_point + '3-' + model_name + '_' + accuracy_percentage + '-' + 'roc_val.svg', format=fformat, dpi=dpi)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve for validation set\n",
    "scikitplot.metrics.plot_precision_recall(Y_test, y_pred)\n",
    "# plt.title(model._name + ' Validation Precision-Recall Curve', fontproperties=font)\n",
    "plt.title('')\n",
    "plt.xlabel('Recall', fontproperties=font)\n",
    "plt.ylabel('Precision', fontproperties=font)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_save_point + '3-' + model_name + '_' + accuracy_percentage + '-' + 'pr_val.svg', format=fformat, dpi=dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the ROC and PR curves for the test data.\n",
    "\n",
    "# Plot ROC curve for test set\n",
    "scikitplot.metrics.plot_roc(test_labels, test_pred)\n",
    "# plt.title(model._name + ' Test ROC Curve', fontproperties=font)\n",
    "plt.title('')\n",
    "plt.xlabel('Specificity', fontproperties=font)\n",
    "plt.ylabel('Sensitivity', fontproperties=font)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_save_point + '4-' + model_name + '_' + accuracy_percentage + '-' + 'roc_test.svg', format=fformat, dpi=dpi)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve for test set\n",
    "scikitplot.metrics.plot_precision_recall(test_labels, test_pred)\n",
    "# plt.title(model._name + ' Test Precision-Recall Curve', fontproperties=font)\n",
    "plt.title('')\n",
    "plt.xlabel('Recall', fontproperties=font)\n",
    "plt.ylabel('Precision', fontproperties=font)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_save_point + '4-' + model_name + '_' + accuracy_percentage + '-' + 'pr_test.svg', format=fformat, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the overall scores.\n",
    "precision = np.zeros(cm_test.shape[0])\n",
    "recall = np.zeros(cm_test.shape[0])\n",
    "f1 = np.zeros(cm_test.shape[0])\n",
    "\n",
    "for i in range(cm_test.shape[0]):\n",
    "    col_sum = np.sum(cm_test[:, i])\n",
    "    if col_sum == 0:\n",
    "        precision[i] = 0\n",
    "        recall[i] = 0\n",
    "        f1[i] = 0\n",
    "    else:\n",
    "        precision[i] = cm_test[i, i] / col_sum\n",
    "        recall[i] = cm_test[i, i] / np.sum(cm_test[i, :])\n",
    "        f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "\n",
    "precision_mean = np.mean(precision)\n",
    "recall_mean = np.mean(recall)\n",
    "f1_mean = np.mean(f1)\n",
    "test_acc = test_scores['accuracy']\n",
    "\n",
    "# Format the precision, recall, and f1-score values as percentages with 2 decimal points\n",
    "precision_mean_percentage = \"{:.2%}\".format(precision_mean)\n",
    "recall_mean_percentage = \"{:.2%}\".format(recall_mean)\n",
    "f1_mean_percentage = \"{:.2%}\".format(f1_mean)\n",
    "accuracy_percentage = \"{:.2%}\".format(test_acc)\n",
    "\n",
    "print(f'Test Scores for {model_name}:')\n",
    "print('-'*len(model_name)*2)\n",
    "print(f'Accuracy: {accuracy_percentage}')\n",
    "print(f'Precision: {precision_mean_percentage}')\n",
    "print(f'Recall: {recall_mean_percentage}')\n",
    "print(f'F1-score: {f1_mean_percentage}')\n",
    "print(f'FLOPS: {flops} GFlops')\n",
    "print(f'Parameters: {params} M')\n",
    "print('-'*len(model_name)*2)\n",
    "\n",
    "def check_if_content_exists(file_path, content):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if content in line:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Save the output to a text file\n",
    "with open('test_scores.txt', 'a') as f:\n",
    "    if not check_if_content_exists('test_scores.txt', f'Test Scores for {model_name}:\\n'):\n",
    "        f.write(f'Test Scores for {model_name}:\\n')\n",
    "        f.write('-'*len(model_name)*2 + '\\n')\n",
    "        f.write(f'Accuracy: {accuracy_percentage}\\n') \n",
    "        f.write(f'Precision: {precision_mean_percentage}\\n')\n",
    "        f.write(f'Recall: {recall_mean_percentage}\\n')\n",
    "        f.write(f'F1-score: {f1_mean_percentage}\\n')\n",
    "        f.write(f'FLOPS: {flops} GFLOPs\\n')\n",
    "        f.write(f'Parameters: {params} M\\n')\n",
    "        f.write('-'*len(model_name)*2 + '\\n')\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "# Load the text file and convert it to an image\n",
    "with open('test_scores.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "font = ImageFont.truetype('arial.ttf', 16)\n",
    "img = Image.new('RGB', (500, 200), color=(255, 255, 255))\n",
    "draw = ImageDraw.Draw(img)\n",
    "draw.text((20, 20), text, font=font, fill=(0, 0, 0))\n",
    "\n",
    "# Save the image as a PDF\n",
    "img.save(fig_save_point + '5-' + model_name + '_' + accuracy_percentage + '-' + 'summary.pdf', format='PDF', dpi=(300,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and subplots with specified size\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Loss Curves\n",
    "ax[0].plot(history['loss'],'#ff7f0e',linewidth=3.0)\n",
    "ax[0].plot(history['val_loss'],'#1f77b4',linewidth=3.0)\n",
    "\n",
    "# Add shaded area for loss error\n",
    "loss_error = np.std(history['val_loss'])\n",
    "ax[0].fill_between(range(len(history['val_loss'])), \n",
    "                   history['val_loss']-loss_error, \n",
    "                   history['val_loss']+loss_error, \n",
    "                   color='#ff7f0e', \n",
    "                   alpha=0.2)\n",
    "\n",
    "# Add markers for lowest loss\n",
    "min_loss_index = history['val_loss'].index(min(history['val_loss']))\n",
    "ax[0].scatter(min_loss_index, \n",
    "              min(history['val_loss']), \n",
    "              c='#2ca02c', \n",
    "              s=100)\n",
    "\n",
    "ax[0].legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "ax[0].set_xlabel('Epochs ',fontsize=10)\n",
    "ax[0].set_ylabel('Loss',fontsize=10)\n",
    "ax[0].set_title('Loss Curves',fontsize=12)\n",
    "ax[0].grid(True, linewidth=1)\n",
    "ax[0].tick_params(width=2)\n",
    "\n",
    "for spine in ax[0].spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "# Accuracy Curves\n",
    "ax[1].plot(history['accuracy'],'#ff7f0e',linewidth=3.0)\n",
    "ax[1].plot(history['val_accuracy'],'#1f77b4',linewidth=3.0)\n",
    "\n",
    "# Add shaded area for accuracy error\n",
    "acc_error = np.std(history['val_accuracy'])\n",
    "ax[1].fill_between(range(len(history['val_accuracy'])), \n",
    "                   history['val_accuracy']-acc_error, \n",
    "                   history['val_accuracy']+acc_error, \n",
    "                   color='#ff7f0e', \n",
    "                   alpha=0.2)\n",
    "\n",
    "# Add markers for highest accuracy\n",
    "max_acc_index = history['val_accuracy'].index(max(history['val_accuracy']))\n",
    "ax[1].scatter(max_acc_index, max(history['val_accuracy']), c='#2ca02c', s=100)\n",
    "\n",
    "# Annotate the highest accuracy\n",
    "max_acc_epoch = max_acc_index + 1  # Add 1 because epoch numbering starts at 0\n",
    "ax[1].annotate(f'Highest accuracy: {max(history[\"val_accuracy\"]):.2%} (epoch {max_acc_epoch})',\n",
    "             xy=(max_acc_index, max(history[\"val_accuracy\"])),\n",
    "             xytext=(max_acc_index+10, max(history[\"val_accuracy\"])-0.05),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "ax[1].legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "ax[1].set_xlabel('Epochs',fontsize=10)\n",
    "ax[1].set_ylabel('Accuracy',fontsize=10)\n",
    "ax[1].set_title('Accuracy Curves',fontsize=12)\n",
    "ax[1].grid(True, linewidth=1)\n",
    "ax[1].tick_params(width=2)\n",
    "ax[1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "\n",
    "# Annotate the lowest loss\n",
    "min_loss_index = history['val_loss'].index(min(history['val_loss']))\n",
    "ax[0].annotate(f'Lowest loss: {min(history[\"val_loss\"]):.2%} (epoch {min_loss_index+1})',\n",
    "             xy=(min_loss_index, min(history[\"val_loss\"])),\n",
    "             xytext=(min_loss_index+10, min(history[\"val_loss\"])-0.05),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "for spine in ax[1].spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
