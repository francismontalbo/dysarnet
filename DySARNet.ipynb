{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DySARNet Trainer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DEPENDENCIES\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "# import model\n",
    "import pickle\n",
    "import logging\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import model_settings as ms\n",
    "from utils.model import create_dysarnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import applications, Model, layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from matplotlib.ticker import FuncFormatter, PercentFormatter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             mean_squared_error, \n",
    "                             mean_squared_log_error, \n",
    "                             classification_report, \n",
    "                             confusion_matrix, \n",
    "                             roc_curve, \n",
    "                             auc)\n",
    "\n",
    "# PREVENT ERROR UNCESSARY MESSAGES\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired dataset to train and task to perform.\n",
    "cwd = os.getcwd()\n",
    "main_ds = os.path.join(cwd, 'ds')\n",
    "\n",
    "task = ['binary', 'severity']\n",
    "ds_folder = ['ua', 'torgo']\n",
    "\n",
    "task_idx = 0  # Change to select binary or severity task\n",
    "ds_folder_idx = 1  # Change to select 'torgo' dataset (0 for 'ua' dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data.\n",
    "selected_ds = os.path.join(main_ds, \n",
    "                           ds_folder[ds_folder_idx], \n",
    "                           f\"{ds_folder[ds_folder_idx]}_{task[task_idx]}\")\n",
    "print(selected_ds)\n",
    "\n",
    "train_data_dir = os.path.join(selected_ds, 'train')\n",
    "validation_data_dir = os.path.join(selected_ds, 'val')\n",
    "test_data_dir = os.path.join(selected_ds, 'test')\n",
    "\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "print(\"*\"*55)\n",
    "print(\"Data folders found!\")\n",
    "print(\"*\"*55)\n",
    "print(f\"Train Path: {train_data_dir}\")\n",
    "print(f\"Validation Path: {validation_data_dir}\")\n",
    "print(f\"Test Path: {test_data_dir}\")\n",
    "print(\"*\"*55)\n",
    "print(f\"The Input size is set to {img_rows} {img_cols} with a depth of {input_shape[-1]}.\") \n",
    "print(\"*\"*55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the architecture name depedning on the model to be trained.\n",
    "architecture_name = \"DySARNet\"\n",
    "model_name = architecture_name + '-' + task[task_idx].title() + '-' + ds_folder[ds_folder_idx].upper()\n",
    "model_path = os.path.join('model/', model_name)\n",
    "\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters.\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data generators.\n",
    "preprocessing = ImageDataGenerator(rescale = 1. / 255)\n",
    "\n",
    "# Set class labels\n",
    "if task_idx == 0:\n",
    "    classes = ['0_Normal', '1_Dysarthria']\n",
    "else:\n",
    "    classes = ['0_Normal', '1_Mild', '2_Moderate', '3_SeverelyModerate', '4_Severe']\n",
    "\n",
    "# Generators\n",
    "train_datagen = preprocessing\n",
    "         \n",
    "val_datagen = preprocessing\n",
    "\n",
    "test_datagen = preprocessing\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=42,\n",
    "        classes=classes)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        classes=classes)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_rows,img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        classes=classes)\n",
    "\n",
    "# CHECK  THE NUMBER OF SAMPLES\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "print(\"Train samples:\", nb_train_samples)\n",
    "print(\"Validation samples:\", nb_validation_samples)\n",
    "print(\"Test samples:\", nb_test_samples)\n",
    "print(f\"TOTAL SAMPLES: {(nb_train_samples + nb_validation_samples + nb_test_samples)}\")\n",
    "\n",
    "if nb_train_samples == 0:\n",
    "    print(\"NO DATA TRAIN FOUND! Please check your train data path and folders!\")\n",
    "else:\n",
    "    print(\"Train samples found!\")\n",
    "    \n",
    "if nb_validation_samples == 0:\n",
    "    print(\"NO DATA VALIDATION FOUND! Please check your validation data path and folders!\")\n",
    "    print(\"Check the data folders first!\")\n",
    "else:\n",
    "    print(\"Validation samples found!\")\n",
    "    \n",
    "if nb_test_samples == 0:\n",
    "    print(\"NO DATA TEST FOUND! Please check your test data path and folders!\")\n",
    "    print(\"Check the data folders first!\")\n",
    "else:\n",
    "    print(\"Test samples found!\")\n",
    "\n",
    "# check the class indices\n",
    "train_generator.class_indices\n",
    "validation_generator.class_indices\n",
    "test_generator.class_indices\n",
    "\n",
    "# true labels\n",
    "Y_test=validation_generator.classes\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "num_classes= len(train_generator.class_indices)\n",
    "\n",
    "print('Model set to train', num_classes, 'classes')\n",
    "\n",
    "if nb_train_samples and nb_validation_samples and nb_test_samples > 0:\n",
    "    print(\"Generators are set!\")\n",
    "    print(\"Check if dataset is complete and has no problems before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto selects the activation based on the number of classes.\n",
    "\n",
    "if num_classes > 2:\n",
    "    class_activation=\"softmax\"\n",
    "elif num_classes <= 2:\n",
    "    class_activation=\"sigmoid\"\n",
    "\n",
    "print(f\"The class activation is set to {class_activation} due to the classes being {num_classes}.\")\n",
    "print(f\"\\nThe classes to be classified are {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the DySARNet model.\n",
    "model = create_dysarnet(input_shape=(224,224,3), \n",
    "                                        activation=ms.activation,\n",
    "                                        growth_rate=ms.growth_rate, \n",
    "                                        bottleneck_size=ms.bottleneck_size, \n",
    "                                        nb_dense_block=ms.nb_dense_block, \n",
    "                                        nb_filter=ms.nb_filter,\n",
    "                                        model_name=model_name,\n",
    "                                        class_activation=class_activation,\n",
    "                                        num_classes=num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model file\n",
    "model_dir = 'model/'\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    print('Model directory', model_dir, 'successfully created')\n",
    "else:\n",
    "    print('Model directory already exist, no new directory made.')\n",
    "\n",
    "print()\n",
    "print('-'*49)\n",
    "print('Model directory is available for saving the model!')\n",
    "print('-'*49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# Train the model.\n",
    "start_time = time.time()\n",
    "print(f\"{model_name} started training!\\n\")\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    validation_data=validation_generator, \n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    validation_steps=nb_validation_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "print()\n",
    "print(\"Training Time:\")\n",
    "print(\"--------------\")\n",
    "print(f\"Elapsed time in seconds: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Elapsed time in minutes: {elapsed_time / 60:.2f} minutes\")\n",
    "print(f\"Elapsed time in hours: {elapsed_time / 3600:.2f} hours\")\n",
    "print()\n",
    "print(f\"Total Training Time: {train_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial validations\n",
    "val_scores = model.evaluate(validation_generator, return_dict=True, verbose=1)\n",
    "\n",
    "val_acc = val_scores['accuracy'] * 100\n",
    "val_loss = val_scores['loss'] * 100\n",
    "\n",
    "print(f\"\\nValidation Results of {model_name}\")\n",
    "print(f\"Val accuracy: {val_acc:.2f}%\")\n",
    "print(f\"Val loss: {val_loss:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform initial testing\n",
    "test_scores = model.evaluate(test_generator, return_dict=True, verbose=1)\n",
    "\n",
    "test_acc = test_scores['accuracy'] * 100\n",
    "test_loss = test_scores['loss'] * 100\n",
    "\n",
    "print(f\"\\nTest Results of {model_name}\")\n",
    "print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Test loss: {test_loss:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model if satisfied.\n",
    "print(f\"MODEL {model_name} SERIALIZING WAIT FOR A MOMENT...\")\n",
    "print()\n",
    "model.save('model/' + model._name + '/' + model._name + \"-\" + f\"{test_acc:.2f}\" + '.h5')\n",
    "print(f\"Model serialized as {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history for future evaluation.\n",
    "def save_h(file, history):\n",
    "    with open(file + '/' + model._name + '/' + model_name + \"-\" + f\"{test_acc:.2f}\" + '.history', 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "    print(f\"{model._name} history saved!\")\n",
    "\n",
    "save_h('model/', history.history)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f\"The Model weights and history of {model._name} are successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the validation set.\n",
    "y_pred = model.predict(validation_generator, \n",
    "                       nb_validation_samples/batch_size, \n",
    "                       workers=1, \n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the test set.\n",
    "test_pred = model.predict(test_generator, \n",
    "                          nb_validation_samples/batch_size, \n",
    "                          workers=1, \n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the test accuracy percentage to name the model\n",
    "test_acc = test_scores['accuracy']\n",
    "accuracy_percentage = \"{:.2%}\".format(test_acc)\n",
    "\n",
    "print(f'The model achieved a test accuracy of: {accuracy_percentage}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
